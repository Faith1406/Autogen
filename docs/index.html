<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kriteya project</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="logo"><img src="logo.png"></div>
    <h2>Introduction</h2>
    <div class="introduction">
        <p>As artificial intelligence continues to evolve, the concept of multi-agent systems—where multiple intelligent agents collaborate to solve tasks—has gained significant traction. When powered by Large Language Models (LLMs) such as OpenAI’s GPT or Anthropic’s Claude, these agents can perform remarkably human-like reasoning, discussion, and delegation.</p>

        <p>Frameworks like Autogen by Microsoft offer the tools needed to create such systems, enabling the orchestration of LLM-powered agents in a conversation-like setup. These agents can take on various roles (e.g., Developer, Reviewer, Analyst) and work together toward a shared goal.</p>
     
        <p> However, while Autogen is powerful, building multi-agent systems with it typically involves writing a lot of boilerplate code. Users must manage individual agents, assign them system messages, choose LLM providers, set response limits, define stopping conditions, and coordinate interactions. For every experiment or prototype, much of this must be reconfigured from scratch—making it hard to iterate quickly or scale up solutions.To tackle this,we use Palette.</p>
    </div>
    <h2>Installation</h2>
    <div class="install">
        <p>pip Install Palette</p>
    </div>
    <h2>Code Explanation</h2>
    <div class="code">
        <pre><code class="language-python">
            import asyncio
            # this is the model factory 
            from model_factory import get_model_client 
            # other team chats imports
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.base import TaskResult
            from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_agentchat.ui import Console
            from autogen_core import CancellationToken
            </code></pre>
    </div>
    <h2>Performance Analysis</h2>
    <div class="performance">
        <P>Palette is designed with efficiency, modularity, and scalability in mind. By building on top of Microsoft's Autogen, it inherits a solid foundation while removing much of the friction involved in multi-agent orchestration. Below is a breakdown of how Palette performs across key dimensions:</P>
        <h3>1. Reduced Development Time</h3>
        <p>Before Palette: Developers spent considerable time configuring each agent—setting models, prompts, termination logic, and interaction rules manually.
        With AutoEgent: Setting up a multi-agent team takes only a few lines of code using simplified constructors and predefined configurations.</p>
        <h3>Result:</h3>
        <p>
        Up to 70–80% reduction in initial setup time.
        Drastically faster iteration speed when testing new ideas or refining agent roles.</p>

        <h3>2. Optimized Execution Flow</h3>
        <p>AutoEgent includes a built-in conversation orchestrator that manages agent interactions more efficiently. It handles:

            Parallel processing for non-dependent agents,
            
            Intelligent message routing to avoid redundant exchanges,
            
            Early stopping once a satisfactory result is achieved.</p>
        <h3>Result:</h3>
        <p>Fewer token exchanges, reducing LLM API costs.

            Faster conversation cycles, especially in scenarios with 3+ agents.</p>

        <h3>3. Scalability</h3>
        <p>AutoEgent supports plug-and-play agent expansion—teams of 3+ agents can be coordinated without introducing extra boilerplate.

            It allows dynamic agent spawning or retirement based on conversation context or logic rules.</p>
        <h3>Result:</h3>
        <p>Smooth scaling from small experiments to complex multi-agent simulations.

            Maintains consistent performance across varying agent counts.</p>

        <h3>4. Model Flexibility</h3>
        <p>AutoEgent supports multiple LLM backends (OpenAI, Anthropic, Azure, etc.) and enables runtime switching. You can assign different agents to different models based on:
            
            Token efficiency
            
            Response speed
            
            Role criticality</p>
        <h3>Result:</h3>
        <p>Better resource utilization by mixing lightweight models for simple roles and powerful models for core logic agents.

            Ability to benchmark LLMs within the same conversation setup.</p>
 
    </div>
    <div class="flowchart">
        <img src="flowchart.jpeg">
    </div>
    <div class="architecture">
        <p>Palette Core is the central controller that initializes two agents—Primary LLM (task performer) and Critic LLM (reviewer). These agents interact using a Round-Robin Conversation Engine, taking turns to exchange messages.
                The Conversation Engine manages the dialogue flow and context. It keeps the interaction going until a Termination Condition is met—either through an ExternalTrigger (manual/programmed) or a MentionTrigger (like the critic saying “Approved”). 
                The final result is displayed via the Output Console (terminal, log, or dashboard). This setup enables intelligent, role-based collaboration between agents with minimal setup and clean modularity</p>
    </div>
    <h2>Uses of Palette</h2>
  <div class="uses">
    <div>
        <h3>1. Rapid Multi-Agent Prototyping</h3>
        <p>Easily set up multiple LLM-powered agents (e.g., Developer, Reviewer, Analyst) with minimal code.</p>
    </div>
    <div>
        <h3>2. Simplified Agent Configuration</h3>
        <p>Abstracts away complex Autogen setup—no need to manually manage system messages, LLM providers, or stopping conditions.</p>
    </div>
    <div>
        <h3> 3. Cross-Model Experimentation</h3>
        <p>Supports different LLMs (OpenAI, Anthropic, etc.) within the same environment for quick testing and comparison.</p>
    </div>
    <div>
        <h3>4. Dynamic Collaboration Flows</h3>
        <p>Allows agents to communicate, delegate tasks, and reason together—ideal for workflows like coding, research, analysis, and decision-making.</p>
    </div>
    <div>
        <h3>5. Reusable & Modular Design</h3>
        <p>Build templates for agent roles and reuse them across different projects or tasks without rewriting logic.</p>
    </div>
    <div>
        <h3> 6. Optimized Performance & Cost</h3>
        <div>Reduces token usage and improves efficiency by managing message flow, early stopping, and minimizing redundancy.</div>
    </div>
  </div>
    